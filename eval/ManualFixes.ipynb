{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74649194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "from sal.utils.parser import H4ArgumentParser\n",
    "from sal.config import Config\n",
    "from datasets import Dataset, load_dataset\n",
    "from sal.utils.data import get_dataset, save_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1180a307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi 1 time\n"
     ]
    }
   ],
   "source": [
    "print('hi 1 time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03eded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7556db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 25 examples [00:00, 137.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id', 'completions', 'scores', 'pred', 'completion_tokens'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "load_dataset('/cluster/tufts/hugheslab/kheuto01/code/probabilistic-inference-scaling/outputs/20250613_232749_task16/', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532da0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config object created successfully!\n",
      "Dataset range: 0 to 100\n",
      "Push to hub: True\n",
      "Hub dataset ID: your-username/your-dataset-name\n"
     ]
    }
   ],
   "source": [
    "# Create Config object with specified fields\n",
    "config = Config(\n",
    "    dataset_start=0,           # Starting index for dataset processing\n",
    "    dataset_end=25,           # Ending index for dataset processing  \n",
    "    push_to_hub=True,          # Enable pushing results to HuggingFace Hub\n",
    "    hub_dataset_id=\"your-username/your-dataset-name\"  # Replace with your actual hub dataset ID\n",
    ")\n",
    "\n",
    "print(\"Config object created successfully!\")\n",
    "print(f\"Dataset range: {config.dataset_start} to {config.dataset_end}\")\n",
    "print(f\"Push to hub: {config.push_to_hub}\")\n",
    "print(f\"Hub dataset ID: {config.hub_dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f21aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from all tasks...\n",
      "================================================================================\n",
      "  Task  1 batch_0: 5 samples\n",
      "  Task  1 batch_1: 5 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task  1 batch_2: 5 samples\n",
      "  Task  1 batch_3: 5 samples\n",
      "  Task  1 batch_4: 5 samples\n",
      "  Task  2 batch_0: 5 samples\n",
      "  Task  2 batch_1: 5 samples\n",
      "  Task  2 batch_2: 5 samples\n",
      "  Task  2 batch_3: 5 samples\n",
      "  Task  2 batch_4: 5 samples\n",
      "  Task  2 batch_1: 5 samples\n",
      "  Task  2 batch_2: 5 samples\n",
      "  Task  2 batch_3: 5 samples\n",
      "  Task  2 batch_4: 5 samples\n",
      "  Task  3 batch_0: 5 samples\n",
      "  Task  3 batch_1: 5 samples\n",
      "  Task  3 batch_2: 5 samples\n",
      "  Task  3 batch_0: 5 samples\n",
      "  Task  3 batch_1: 5 samples\n",
      "  Task  3 batch_2: 5 samples\n",
      "  Task  3 batch_3: 5 samples\n",
      "  Task  3 batch_4: 5 samples\n",
      "  Task  4 batch_0: 5 samples\n",
      "  Task  4 batch_1: 5 samples\n",
      "  Task  4 batch_2: 5 samples\n",
      "  Task  3 batch_3: 5 samples\n",
      "  Task  3 batch_4: 5 samples\n",
      "  Task  4 batch_0: 5 samples\n",
      "  Task  4 batch_1: 5 samples\n",
      "  Task  4 batch_2: 5 samples\n",
      "  Task  4 batch_3: 5 samples\n",
      "  Task  4 batch_4: 5 samples\n",
      "  Task  5 batch_0: 5 samples\n",
      "  Task  5 batch_1: 5 samples\n",
      "  Task  5 batch_2: 5 samples\n",
      "  Task  4 batch_3: 5 samples\n",
      "  Task  4 batch_4: 5 samples\n",
      "  Task  5 batch_0: 5 samples\n",
      "  Task  5 batch_1: 5 samples\n",
      "  Task  5 batch_2: 5 samples\n",
      "  Task  5 batch_3: 5 samples\n",
      "  Task  5 batch_4: 5 samples\n",
      "  Task  6 batch_0: 5 samples\n",
      "  Task  6 batch_1: 5 samples\n",
      "  Task  5 batch_3: 5 samples\n",
      "  Task  5 batch_4: 5 samples\n",
      "  Task  6 batch_0: 5 samples\n",
      "  Task  6 batch_1: 5 samples\n",
      "  Task  6 batch_2: 5 samples\n",
      "  Task  6 batch_3: 5 samples\n",
      "  Task  6 batch_4: 5 samples\n",
      "  Task  7 batch_0: 5 samples\n",
      "  Task  7 batch_1: 5 samples\n",
      "  Task  6 batch_2: 5 samples\n",
      "  Task  6 batch_3: 5 samples\n",
      "  Task  6 batch_4: 5 samples\n",
      "  Task  7 batch_0: 5 samples\n",
      "  Task  7 batch_1: 5 samples\n",
      "  Task  7 batch_2: 5 samples\n",
      "  Task  7 batch_3: 5 samples\n",
      "  Task  7 batch_4: 5 samples\n",
      "  Task  8 batch_0: 5 samples\n",
      "  Task  7 batch_2: 5 samples\n",
      "  Task  7 batch_3: 5 samples\n",
      "  Task  7 batch_4: 5 samples\n",
      "  Task  8 batch_0: 5 samples\n",
      "  Task  8 batch_1: 5 samples\n",
      "  Task  8 batch_2: 5 samples\n",
      "  Task  8 batch_3: 5 samples\n",
      "  Task  8 batch_4: 5 samples\n",
      "  Task  8 batch_1: 5 samples\n",
      "  Task  8 batch_2: 5 samples\n",
      "  Task  8 batch_3: 5 samples\n",
      "  Task  8 batch_4: 5 samples\n",
      "  Task  9 batch_0: 5 samples\n",
      "  Task  9 batch_1: 5 samples\n",
      "  Task  9 batch_2: 5 samples\n",
      "  Task  9 batch_3: 5 samples\n",
      "  Task  9 batch_4: 5 samples\n",
      "  Task  9 batch_0: 5 samples\n",
      "  Task  9 batch_1: 5 samples\n",
      "  Task  9 batch_2: 5 samples\n",
      "  Task  9 batch_3: 5 samples\n",
      "  Task  9 batch_4: 5 samples\n",
      "  Task 10 batch_0: 5 samples\n",
      "  Task 10 batch_1: 5 samples\n",
      "  Task 10 batch_2: 5 samples\n",
      "  Task 10 batch_3: 5 samples\n",
      "  Task 10 batch_0: 5 samples\n",
      "  Task 10 batch_1: 5 samples\n",
      "  Task 10 batch_2: 5 samples\n",
      "  Task 10 batch_3: 5 samples\n",
      "  Task 10 batch_4: 5 samples\n",
      "  Task 11 batch_0: 5 samples\n",
      "  Task 11 batch_1: 5 samples\n",
      "  Task 11 batch_2: 5 samples\n",
      "  Task 11 batch_3: 5 samples\n",
      "  Task 10 batch_4: 5 samples\n",
      "  Task 11 batch_0: 5 samples\n",
      "  Task 11 batch_1: 5 samples\n",
      "  Task 11 batch_2: 5 samples\n",
      "  Task 11 batch_3: 5 samples\n",
      "  Task 11 batch_4: 5 samples\n",
      "  Task 12 batch_0: 5 samples\n",
      "  Task 12 batch_1: 5 samples\n",
      "  Task 12 batch_2: 5 samples\n",
      "  Task 11 batch_4: 5 samples\n",
      "  Task 12 batch_0: 5 samples\n",
      "  Task 12 batch_1: 5 samples\n",
      "  Task 12 batch_2: 5 samples\n",
      "  Task 12 batch_3: 5 samples\n",
      "  Task 12 batch_4: 5 samples\n",
      "  Task 13 batch_0: 5 samples\n",
      "  Task 13 batch_1: 5 samples\n",
      "  Task 13 batch_2: 5 samples\n",
      "  Task 12 batch_3: 5 samples\n",
      "  Task 12 batch_4: 5 samples\n",
      "  Task 13 batch_0: 5 samples\n",
      "  Task 13 batch_1: 5 samples\n",
      "  Task 13 batch_2: 5 samples\n",
      "  Task 13 batch_3: 5 samples\n",
      "  Task 13 batch_4: 5 samples\n",
      "  Task 14 batch_0: 5 samples\n",
      "  Task 14 batch_1: 5 samples\n",
      "  Task 14 batch_2: 5 samples\n",
      "  Task 13 batch_3: 5 samples\n",
      "  Task 13 batch_4: 5 samples\n",
      "  Task 14 batch_0: 5 samples\n",
      "  Task 14 batch_1: 5 samples\n",
      "  Task 14 batch_2: 5 samples\n",
      "  Task 14 batch_3: 5 samples\n",
      "  Task 14 batch_4: 5 samples\n",
      "  Task 15 batch_0: 5 samples\n",
      "  Task 15 batch_1: 5 samples\n",
      "  Task 15 batch_2: 5 samples\n",
      "  Task 14 batch_3: 5 samples\n",
      "  Task 14 batch_4: 5 samples\n",
      "  Task 15 batch_0: 5 samples\n",
      "  Task 15 batch_1: 5 samples\n",
      "  Task 15 batch_2: 5 samples\n",
      "  Task 15 batch_3: 5 samples\n",
      "  Task 15 batch_4: 5 samples\n",
      "  Task 17 batch_0: 5 samples\n",
      "  Task 17 batch_1: 5 samples\n",
      "  Task 17 batch_2: 5 samples\n",
      "  Task 15 batch_3: 5 samples\n",
      "  Task 15 batch_4: 5 samples\n",
      "  Task 17 batch_0: 5 samples\n",
      "  Task 17 batch_1: 5 samples\n",
      "  Task 17 batch_2: 5 samples\n",
      "  Task 17 batch_3: 5 samples\n",
      "  Task 17 batch_4: 5 samples\n",
      "  Task 18 batch_0: 5 samples\n",
      "  Task 18 batch_1: 5 samples\n",
      "  Task 17 batch_3: 5 samples\n",
      "  Task 17 batch_4: 5 samples\n",
      "  Task 18 batch_0: 5 samples\n",
      "  Task 18 batch_1: 5 samples\n",
      "  Task 18 batch_2: 5 samples\n",
      "  Task 18 batch_3: 5 samples\n",
      "  Task 18 batch_4: 5 samples\n",
      "  Task 19 batch_0: 5 samples\n",
      "  Task 19 batch_1: 5 samples\n",
      "  Task 18 batch_2: 5 samples\n",
      "  Task 18 batch_3: 5 samples\n",
      "  Task 18 batch_4: 5 samples\n",
      "  Task 19 batch_0: 5 samples\n",
      "  Task 19 batch_1: 5 samples\n",
      "  Task 19 batch_2: 5 samples\n",
      "  Task 19 batch_3: 5 samples\n",
      "  Task 19 batch_4: 5 samples\n",
      "  Task 20 batch_0: 5 samples\n",
      "  Task 19 batch_2: 5 samples\n",
      "  Task 19 batch_3: 5 samples\n",
      "  Task 19 batch_4: 5 samples\n",
      "  Task 20 batch_0: 5 samples\n",
      "  Task 20 batch_1: 5 samples\n",
      "  Task 20 batch_2: 5 samples\n",
      "  Task 20 batch_3: 5 samples\n",
      "  Task 20 batch_4: 5 samples\n",
      "  Task 20 batch_1: 5 samples\n",
      "  Task 20 batch_2: 5 samples\n",
      "  Task 20 batch_3: 5 samples\n",
      "  Task 20 batch_4: 5 samples\n",
      "  Task 16 batch_0: 5 samples\n",
      "  Task 16 batch_1: 5 samples\n",
      "  Task 16 batch_2: 5 samples\n",
      "  Task 16 batch_3: 5 samples\n",
      "  Task 16 batch_0: 5 samples\n",
      "  Task 16 batch_1: 5 samples\n",
      "  Task 16 batch_2: 5 samples\n",
      "  Task 16 batch_3: 5 samples\n",
      "  Task 16 batch_4: 5 samples\n",
      "\n",
      "================================================================================\n",
      "COMBINING ALL DATASETS\n",
      "================================================================================\n",
      "Total datasets to combine: 100\n",
      "Concatenating datasets...\n",
      "Combined dataset size: 500 samples\n",
      "Saving combined dataset to: /cluster/tufts/hugheslab/kheuto01/code/probabilistic-inference-scaling/outputs/kheuton_Qwen2.5-1.5B-Instruct-bon-completions_combined.json\n",
      "Converting to JSON format...\n",
      "  Task 16 batch_4: 5 samples\n",
      "\n",
      "================================================================================\n",
      "COMBINING ALL DATASETS\n",
      "================================================================================\n",
      "Total datasets to combine: 100\n",
      "Concatenating datasets...\n",
      "Combined dataset size: 500 samples\n",
      "Saving combined dataset to: /cluster/tufts/hugheslab/kheuto01/code/probabilistic-inference-scaling/outputs/kheuton_Qwen2.5-1.5B-Instruct-bon-completions_combined.json\n",
      "Converting to JSON format...\n",
      "Writing to file...\n",
      "Writing to file...\n",
      "✅ Successfully saved 500 samples to kheuton_Qwen2.5-1.5B-Instruct-bon-completions_combined.json\n",
      "📁 File location: /cluster/tufts/hugheslab/kheuto01/code/probabilistic-inference-scaling/outputs/kheuton_Qwen2.5-1.5B-Instruct-bon-completions_combined.json\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Subdirectories processed: 20\n",
      "Individual batch files loaded: 100\n",
      "Tasks processed: 20\n",
      "================================================================================\n",
      "Task  1: [  0- 25] -> 5/5 batches loaded\n",
      "Task  2: [ 25- 50] -> 5/5 batches loaded\n",
      "Task  3: [ 50- 75] -> 5/5 batches loaded\n",
      "Task  4: [ 75-100] -> 5/5 batches loaded\n",
      "Task  5: [100-125] -> 5/5 batches loaded\n",
      "...\n",
      "Task 17: [400-425] -> 5/5 batches loaded\n",
      "Task 18: [425-450] -> 5/5 batches loaded\n",
      "Task 19: [450-475] -> 5/5 batches loaded\n",
      "Task 20: [475-500] -> 5/5 batches loaded\n",
      "Task 16: [375-400] -> 5/5 batches loaded\n",
      "================================================================================\n",
      "✅ Successfully saved 500 samples to kheuton_Qwen2.5-1.5B-Instruct-bon-completions_combined.json\n",
      "📁 File location: /cluster/tufts/hugheslab/kheuto01/code/probabilistic-inference-scaling/outputs/kheuton_Qwen2.5-1.5B-Instruct-bon-completions_combined.json\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Subdirectories processed: 20\n",
      "Individual batch files loaded: 100\n",
      "Tasks processed: 20\n",
      "================================================================================\n",
      "Task  1: [  0- 25] -> 5/5 batches loaded\n",
      "Task  2: [ 25- 50] -> 5/5 batches loaded\n",
      "Task  3: [ 50- 75] -> 5/5 batches loaded\n",
      "Task  4: [ 75-100] -> 5/5 batches loaded\n",
      "Task  5: [100-125] -> 5/5 batches loaded\n",
      "...\n",
      "Task 17: [400-425] -> 5/5 batches loaded\n",
      "Task 18: [425-450] -> 5/5 batches loaded\n",
      "Task 19: [450-475] -> 5/5 batches loaded\n",
      "Task 20: [475-500] -> 5/5 batches loaded\n",
      "Task 16: [375-400] -> 5/5 batches loaded\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Combine all batch datasets into one JSON file\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Base paths and configuration\n",
    "outputs_base_dir = \"/cluster/tufts/hugheslab/kheuto01/code/probabilistic-inference-scaling/outputs\"\n",
    "hub_dataset_id = \"kheuton/Qwen2.5-1.5B-Instruct-bon-completions\"\n",
    "\n",
    "# Get all subdirectories in outputs\n",
    "subdirs = [d for d in os.listdir(outputs_base_dir) if os.path.isdir(os.path.join(outputs_base_dir, d))]\n",
    "subdirs.sort()\n",
    "\n",
    "# Collect all datasets\n",
    "all_datasets = []  # Store all datasets to combine later\n",
    "task_info = []\n",
    "loaded_datasets = 0\n",
    "\n",
    "print(\"Loading datasets from all tasks...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for s, subdir in enumerate(subdirs):\n",
    "    # Extract task number from directory name (format: YYYYMMDD_#####_task#)\n",
    "    match = re.search(r'task(\\d+)', subdir)\n",
    "    if match:\n",
    "        task_num = int(match.group(1))\n",
    "        \n",
    "        # Calculate dataset range: task 1 = 0-25, task 2 = 25-50, etc.\n",
    "        dataset_start = (task_num - 1) * 25\n",
    "        dataset_end = task_num * 25\n",
    "        \n",
    "        # Full path to the output directory\n",
    "        output_dir = os.path.join(outputs_base_dir, subdir)\n",
    "        \n",
    "        # Load batch files for this task\n",
    "        batch_count = 0\n",
    "        for batch_num in range(5):  # batch_0.jsonl through batch_4.jsonl\n",
    "            batch_file = os.path.join(output_dir, f\"batch_{batch_num}.jsonl\")\n",
    "            if os.path.exists(batch_file):\n",
    "                try:\n",
    "                    dataset = load_dataset('json', data_files=batch_file, split='train')\n",
    "                    all_datasets.append(dataset)  # Add to overall collection\n",
    "                    batch_count += 1\n",
    "                    loaded_datasets += 1\n",
    "                    print(f\"  Task {task_num:2d} batch_{batch_num}: {len(dataset)} samples\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error loading {batch_file}: {e}\")\n",
    "            else:\n",
    "                print(f\"  Warning: {batch_file} not found\")\n",
    "        \n",
    "        task_info.append(f\"Task {task_num:2d}: [{dataset_start:3d}-{dataset_end:3d}] -> {batch_count}/5 batches loaded\")\n",
    "\n",
    "# Combine all datasets into one\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMBINING ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total datasets to combine: {len(all_datasets)}\")\n",
    "\n",
    "if all_datasets:\n",
    "    # Concatenate all datasets\n",
    "    print(\"Concatenating datasets...\")\n",
    "    combined_dataset = concatenate_datasets(all_datasets)\n",
    "    print(f\"Combined dataset size: {len(combined_dataset)} samples\")\n",
    "    \n",
    "    # Create output filename based on hub dataset id\n",
    "    # Convert hub_dataset_id to a safe filename\n",
    "    safe_filename = hub_dataset_id.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "    output_filename = f\"{safe_filename}_combined.json\"\n",
    "    output_path = os.path.join(outputs_base_dir, output_filename)\n",
    "    \n",
    "    # Save as JSON\n",
    "    print(f\"Saving combined dataset to: {output_path}\")\n",
    "    \n",
    "    # Convert to list of dictionaries and save as JSON\n",
    "    print(\"Converting to JSON format...\")\n",
    "    data_list = [sample for sample in combined_dataset]\n",
    "    \n",
    "    print(\"Writing to file...\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Successfully saved {len(data_list)} samples to {output_filename}\")\n",
    "    print(f\"📁 File location: {output_path}\")\n",
    "else:\n",
    "    print(\"❌ No datasets were loaded to combine\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Subdirectories processed: {len(subdirs)}\")\n",
    "print(f\"Individual batch files loaded: {loaded_datasets}\")\n",
    "print(f\"Tasks processed: {len(task_info)}\")\n",
    "print(\"=\" * 80)\n",
    "for info in task_info[:5]:  # Show first 5\n",
    "    print(info)\n",
    "if len(task_info) > 10:\n",
    "    print(\"...\")\n",
    "    for info in task_info[-5:]:  # Show last 5\n",
    "        print(info)\n",
    "elif len(task_info) > 5:\n",
    "    for info in task_info[5:]:  # Show remaining if <= 10 total\n",
    "        print(info)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "217bb032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "935d9d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST 1: This should appear once\n",
      "TEST 2: Loop iteration 0\n",
      "TEST 2: Loop iteration 1\n",
      "TEST 2: Loop iteration 2\n",
      "TEST 3: This should also appear once\n",
      "Length of subdirs variable: 20\n",
      "Contents of subdirs: ['20250613_125630_task1', '20250613_132720_task2', '20250613_135801_task3', '20250613_142826_task4', '20250613_145911_task5', '20250613_164055_task6', '20250613_171253_task7', '20250613_174451_task8', '20250613_181646_task9', '20250613_184911_task10', '20250613_192138_task11', '20250613_195306_task12', '20250613_202519_task13', '20250613_205535_task14', '20250613_205731_task15', '20250613_212935_task17', '20250613_220024_task18', '20250613_223109_task19', '20250613_230202_task20', '20250613_232749_task16']\n",
      "Unique subdirs count: 20\n",
      "Are there duplicates in subdirs? False\n",
      "\n",
      "--- Simple loop test ---\n",
      "Simple test iteration: 0\n",
      "Simple test iteration: 1\n",
      "Simple test iteration: 2\n",
      "Simple test iteration: 3\n",
      "Simple test iteration: 4\n",
      "Simple test iteration: 5\n",
      "... (truncated)\n",
      "=== Testing Config creation side effects ===\n",
      "Creating Config object...\n",
      "Config created successfully\n",
      "\n",
      "=== Testing loop with Config creation ===\n",
      "Loop iteration 0: before Config\n",
      "Loop iteration 0: after Config\n",
      "Loop iteration 1: before Config\n",
      "Loop iteration 1: after Config\n",
      "Loop iteration 2: before Config\n",
      "Loop iteration 2: after Config\n",
      "=== Test complete ===\n",
      "Total subdirectories found: 20\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n",
      "iteration: 10\n",
      "iteration: 11\n",
      "iteration: 12\n",
      "iteration: 13\n",
      "iteration: 14\n",
      "iteration: 15\n",
      "iteration: 16\n",
      "iteration: 17\n",
      "iteration: 18\n",
      "iteration: 19\n",
      "Total print statements executed: 20\n",
      "Expected: 20, Actual: 20\n",
      "=== BEFORE importing Config ===\n",
      "Root logger handlers: 0\n",
      "\n",
      "=== AFTER importing Config ===\n",
      "Root logger handlers: 0\n",
      "\n",
      "=== BEFORE creating first Config object ===\n",
      "\n",
      "=== AFTER creating first Config object ===\n",
      "Root logger handlers: 0\n",
      "\n",
      "=== BEFORE creating second Config object ===\n",
      "\n",
      "=== AFTER creating second Config object ===\n",
      "Root logger handlers: 0\n",
      "=== Test without Config creation ===\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "\n",
      "=== Test with Config creation (push_to_hub=False) ===\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "\n",
      "Root logger handlers after Config creation: 0\n"
     ]
    }
   ],
   "source": [
    "# Simple test to check for duplicate output\n",
    "print(\"TEST 1: This should appear once\")\n",
    "for i in range(3):\n",
    "    print(f\"TEST 2: Loop iteration {i}\")\n",
    "print(\"TEST 3: This should also appear once\")\n",
    "\n",
    "# Diagnostic test for the duplication issue\n",
    "import os\n",
    "\n",
    "print(f\"Length of subdirs variable: {len(subdirs)}\")\n",
    "print(f\"Contents of subdirs: {subdirs}\")\n",
    "\n",
    "# Test if there are duplicates in subdirs\n",
    "unique_subdirs = list(set(subdirs))\n",
    "print(f\"Unique subdirs count: {len(unique_subdirs)}\")\n",
    "print(f\"Are there duplicates in subdirs? {len(subdirs) != len(unique_subdirs)}\")\n",
    "\n",
    "# Simple loop test with the same subdirs variable\n",
    "print(\"\\n--- Simple loop test ---\")\n",
    "for i, subdir in enumerate(subdirs):\n",
    "    print(f\"Simple test iteration: {i}\")\n",
    "    if i >= 5:  # Only show first 5 to keep output manageable\n",
    "        print(\"... (truncated)\")\n",
    "        break\n",
    "\n",
    "# Test if Config creation has side effects\n",
    "print(\"=== Testing Config creation side effects ===\")\n",
    "\n",
    "# Test creating a single Config object\n",
    "print(\"Creating Config object...\")\n",
    "test_config = Config(\n",
    "    dataset_start=0,\n",
    "    dataset_end=25,\n",
    "    push_to_hub=True,\n",
    "    hub_dataset_id=\"test/dataset\",\n",
    "    output_dir=\"/tmp/test\"\n",
    ")\n",
    "print(\"Config created successfully\")\n",
    "\n",
    "# Test a loop with Config creation\n",
    "print(\"\\n=== Testing loop with Config creation ===\")\n",
    "for i in range(3):\n",
    "    print(f\"Loop iteration {i}: before Config\")\n",
    "    temp_config = Config(\n",
    "        dataset_start=i*25,\n",
    "        dataset_end=(i+1)*25,\n",
    "        push_to_hub=True,\n",
    "        hub_dataset_id=\"test/dataset\",\n",
    "        output_dir=f\"/tmp/test{i}\"\n",
    "    )\n",
    "    print(f\"Loop iteration {i}: after Config\")\n",
    "\n",
    "print(\"=== Test complete ===\")\n",
    "\n",
    "# Minimal test to reproduce the 38-times issue\n",
    "# Please restart the kernel first, then run this cell\n",
    "\n",
    "import os\n",
    "outputs_base_dir = \"/cluster/tufts/hugheslab/kheuto01/code/probabilistic-inference-scaling/outputs\"\n",
    "subdirs = [d for d in os.listdir(outputs_base_dir) if os.path.isdir(os.path.join(outputs_base_dir, d))]\n",
    "subdirs.sort()\n",
    "\n",
    "print(f\"Total subdirectories found: {len(subdirs)}\")\n",
    "\n",
    "# Count how many times this prints\n",
    "counter = 0\n",
    "for s, subdir in enumerate(subdirs):\n",
    "    print(f\"iteration: {s}\")\n",
    "    counter += 1\n",
    "\n",
    "print(f\"Total print statements executed: {counter}\")\n",
    "print(f\"Expected: {len(subdirs)}, Actual: {counter}\")\n",
    "\n",
    "# Test logging handlers before and after Config creation\n",
    "import logging\n",
    "\n",
    "def check_logging_handlers():\n",
    "    root_logger = logging.getLogger()\n",
    "    print(f\"Root logger handlers: {len(root_logger.handlers)}\")\n",
    "    for i, handler in enumerate(root_logger.handlers):\n",
    "        print(f\"  Handler {i}: {type(handler).__name__} - {handler}\")\n",
    "\n",
    "print(\"=== BEFORE importing Config ===\")\n",
    "check_logging_handlers()\n",
    "\n",
    "from sal.config import Config\n",
    "\n",
    "print(\"\\n=== AFTER importing Config ===\")\n",
    "check_logging_handlers()\n",
    "\n",
    "print(\"\\n=== BEFORE creating first Config object ===\")\n",
    "test_config1 = Config(dataset_start=0, dataset_end=25, push_to_hub=False)\n",
    "\n",
    "print(\"\\n=== AFTER creating first Config object ===\")\n",
    "check_logging_handlers()\n",
    "\n",
    "print(\"\\n=== BEFORE creating second Config object ===\")\n",
    "test_config2 = Config(dataset_start=25, dataset_end=50, push_to_hub=False)\n",
    "\n",
    "print(\"\\n=== AFTER creating second Config object ===\")\n",
    "check_logging_handlers()\n",
    "\n",
    "# Simple test to isolate the logging issue\n",
    "import logging\n",
    "\n",
    "# Clear any existing handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "print(\"=== Test without Config creation ===\")\n",
    "for i in range(3):\n",
    "    print(f\"iteration: {i}\")\n",
    "\n",
    "print(\"\\n=== Test with Config creation (push_to_hub=False) ===\")\n",
    "from sal.config import Config\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"iteration: {i}\")\n",
    "    # Create Config without push_to_hub to avoid network calls\n",
    "    config = Config(\n",
    "        dataset_start=i*25, \n",
    "        dataset_end=(i+1)*25, \n",
    "        push_to_hub=False,  # This should avoid the hub-related code\n",
    "        output_dir=f\"/tmp/test{i}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nRoot logger handlers after Config creation: {len(logging.root.handlers)}\")\n",
    "\n",
    "# Test the autoreload + logging theory\n",
    "import logging\n",
    "\n",
    "print(f\"Before: {len(logging.root.handlers)} handlers\")\n",
    "\n",
    "# Create first Config \n",
    "config1 = Config(dataset_start=0, dataset_end=25, push_to_hub=False)\n",
    "print(f\"After Config 1: {len(logging.root.handlers)} handlers\")\n",
    "\n",
    "# Create second Config\n",
    "config2 = Config(dataset_start=25, dataset_end=50, push_to_hub=False) \n",
    "print(f\"After Config 2: {len(logging.root.handlers)} handlers\")\n",
    "\n",
    "# Test print statements\n",
    "print(\"\\n=== Testing print with multiple handlers ===\")\n",
    "for i in range(3):\n",
    "    print(f\"iteration: {i}\")\n",
    "    \n",
    "print(f\"\\nFinal handler count: {len(logging.root.handlers)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
